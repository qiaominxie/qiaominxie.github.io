---
---

@string{aps = {American Physical Society,}}

@article{lu2011join,
  title={Join-idle-queue: A novel load balancing algorithm for dynamically scalable web services},
  author={Lu, Yi and Xie, Qiaomin and Kliot, Gabriel and Geller, Alan and Larus, James R and Greenberg, Albert},
  journal={Performance Evaluation},
  volume={68},
  number={11},
  pages={1056--1071},
  year={2011},
  publisher={North-Holland},
  url={https://dl.acm.org/doi/10.1016/j.peva.2011.07.015}
}

@inproceedings{xie2012degree,
  title={Degree-guided map-reduce task assignment with data locality constraint},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2012 IEEE International Symposium on Information Theory Proceedings},
  pages={985--989},
  year={2012},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/6284711}
}

@inproceedings{xie2015power,
  title={Power of d Choices for Large-Scale Bin Packing: A Loss Model},
  author={Xie, Qiaomin and Dong, Xiaobo and Lu, Yi and Srikant, R},
  booktitle={ACM Sigmetrics},
  year={2015},
  url={https://dl.acm.org/doi/10.1145/2745844.2745849}
}

@inproceedings{xie2015priority,
  title={Priority algorithm for near-data scheduling: Throughput and heavy-traffic optimality},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)},
  pages={963--972},
  year={2015},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/7218468},
}

@inproceedings{xie2016scheduling,
  title={Scheduling with Multi-level Data Locality: Throughput and Heavy-Traffic Optimality},
  author={Xie, Qiaomin and Yekkehkhany, Ali and Lu, Yi},
  booktitle={2016 IEEE Conference on Computer Communications (INFOCOM)},
  year={2016},
  url={https://ieeexplore.ieee.org/document/7524416},
}

@article{xie2016pandas,
  title={Pandas: robust locality-aware scheduling with stochastic delay optimality},
  author={Xie, Qiaomin and Pundir, Mayank and Lu, Yi and Abad, Cristina L and Campbell, Roy H},
  journal={IEEE/ACM Transactions on Networking},
  volume={25},
  number={2},
  pages={662--675},
  year={2016},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/7582519},
}

@inproceedings{gupta2017stochastic,
  title={Stochastic online scheduling on unrelated machines},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  booktitle={International Conference on Integer Programming and Combinatorial Optimization},
  pages={228--240},
  year={2017},
  organization={Springer, Cham},
  url={https://link.springer.com/chapter/10.1007/978-3-319-59250-3_19}
}


@article{shah2017centralized,
  title={Centralized Congestion Control and Scheduling in a Datacenter},
  author={Shah, Devavrat and Xie, Qiaomin},
  journal={arXiv preprint arXiv:1710.02548},
  year={2017},
  arxiv={https://arxiv.org/abs/1710.02548}
}

@inproceedings{shah2018q,
  title={Q-learning with nearest neighbors},
  author={Shah, Devavrat and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018},
  arxiv={https://arxiv.org/abs/1802.03900},
  url={https://proceedings.neurips.cc/paper_files/paper/2018/hash/309fee4e541e51de2e41f21bebb342aa-Abstract.html}
}

@inproceedings{liu2019reinforcement,
  title={Reinforcement learning for optimal control of queueing systems},
  author={Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={663--670},
  year={2019},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/8919665}
}

@inproceedings{fei2020dynamic,
  title={Dynamic Regret of Policy Optimization in Non-Stationary Environments},
  author={Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2007.00148},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/4b0091f82f50ff7095647fe893580d60-Abstract.html}
}

@inproceedings{mao2020poly,
  title={POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis},
  author={Mao, Weichao and Zhang, Kaiqing and Xie, Qiaomin and Basar, Tamer},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.04672},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/30de24287a6d8f07b37c716ad51623a7-Abstract.html},
}

@inproceedings{fei2020risk,
  title={Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret},
  author={Fei, Yingjie and Yang, Zhuoran and Chen, Yudong and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.13827},
  url={https://proceedings.neurips.cc/paper_files/paper/2020/hash/fdc42b6b0ee16a2f866281508ef56730-Abstract.html}
}

@inproceedings{shah2020stable,
  title={Stable Reinforcement Learning with Unbounded State Space},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={Learning for Dynamics and Control (L4DC)},
  pages={581},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.04353}
}

@inproceedings{shah2020reinforcement,
  title={On Reinforcement Learning for Turn-based Zero-sum Markov Games},
  author={Shah, Devavrat and Somani, Varun and Xie, Qiaomin and Xu, Zhi},
  booktitle={Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  pages={pp--139},
  year={2020},
  arxiv={https://arxiv.org/abs/2002.10620}
}


@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on Learning Theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v125/xie20a.html}
}

@inproceedings{shah2020non,
  title={Non-asymptotic analysis of Monte Carlo tree search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={ACM Sigmetrics},
  pages={31--32},
  year={2020},
  url={https://dl.acm.org/doi/abs/10.1145/3393691.3394202}
}

@article{gupta2020greed,
  title={Greed worksâ€”online algorithms for unrelated machine stochastic scheduling},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  journal={Mathematics of operations research},
  volume={45},
  number={2},
  pages={497--516},
  year={2020},
  publisher={INFORMS},
  arixv={https://arxiv.org/abs/1703.01634},
  url={https://pubsonline.informs.org/doi/abs/10.1287/moor.2019.0999},
}

@inproceedings{xie2021learning,
  title={Learning While Playing in Mean-Field Games: Convergence and Optimality},
  author={Xie, Qiaomin and Yang, Zhuoran and Wang, Zhaoran and Minca, Andreea},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={11436--11447},
  year={2021},
  organization={PMLR},
  html={https://proceedings.mlr.press/v139/xie21g.html}
}


@inproceedings{wang2021zero,
  title={Zero queueing for multi-server jobs},
  author={Wang, Weina and Xie, Qiaomin and Harchol-Balter, Mor},
  booktitle={ACM Sigmetrics},
  pages={13--14},
  year={2021},
  arxiv={https://arxiv.org/abs/2011.10521},
  html={https://dl.acm.org/doi/10.1145/3447385}
}

@article{archer2022orsuite,
author = {Archer, Christopher and Banerjee, Siddhartha and Cortez, Mayleen and Rucker, Carrie and Sinclair, Sean R. and Solberg, Max and Xie, Qiaomin and Lee Yu, Christina},
title = {ORSuite: Benchmarking Suite for Sequential Operations Models},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3512798.3512819},
doi = {10.1145/3512798.3512819},
abstract = {Reinforcement learning (RL) has received widespread attention across multiple communities, but the experiments have focused primarily on large-scale game playing and robotics tasks. In this paper we introduce ORSuite, an open-source library containing environments, algorithms, and instrumentation for operational problems. Our package is designed to motivate researchers in the reinforcement learning community to develop and evaluate algorithms on operational tasks, and to consider the true multi-objective nature of these problems by considering metrics beyond cumulative reward.},
journal = {SIGMETRICS Performance Evaluation Review},
month = {jan},
pages = {57--61},
numpages = {5}
}

@article{shah2022non_OR,
  title={Nonasymptotic Analysis of {Monte Carlo} Tree Search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  journal={Operations Research},
  volume={70},
  number={6},
  pages={3234--3260},
  year={2022},
  publisher={INFORMS},
  doi={10.1287/opre.2021.2239},
  html={https://pubsonline.informs.org/doi/abs/10.1287/opre.2021.2239?journalCode=opre}
}

@article{Liu2022rlqn,
author = {Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
title = {RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3529375},
doi = {10.1145/3529375},
abstract = {With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called RL for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing, and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively.},
journal = {ACM Transactions on Modeling and Performance Evaluation of Computing Systems},
month = {aug},
articleno = {2},
numpages = {35},
keywords = {Queueing networks, reinforcement learning}
}

@article{xie2023cce_mor,
author = {Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
title = {Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium},
year = {2023},
issue_date = {February 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {48},
number = {1},
issn = {0364-765X},
url = {https://doi.org/10.1287/moor.2022.1268},
doi = {10.1287/moor.2022.1268},
abstract = {We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an O(d3H3T) upper bound on the duality gap and regret, where d is the linear dimension, H the horizon and T the total number of timesteps. Our results do not require additional assumptions on the sampling model. Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a coarse correlated equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right.},
journal = {Mathematics of Operations Research},
month = {feb},
pages = {433--462},
numpages = {30},
keywords = {Markov games, function approximation, correlated equilibrium, secondary: 91A15, Primary: 68T05, reinforcement learning},
arxiv={https://arxiv.org/abs/2002.07066}
}

@inproceedings{hong2022maximizing,
  title={Near-Optimal Stochastic Bin-Packing in Large Service Systems with Time-Varying Item Sizes},
  author={Hong, Yige and Xie, Qiaomin and Wang, Weina},
  %journal={arXiv preprint arXiv:2209.04123},
  arxiv={https://arxiv.org/abs/2209.04123},
  booktitle={ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
  year={2024},
  month={June}
}

@inproceedings{wu2023reward,
  title={Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning},
  author={Wu, Young and McMahan, Jermey and Zhu, Xiaojin and Xie, Qiaomin},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2023},
  month={February},
  arxiv={https://arxiv.org/abs/2206.01888}
}


@inproceedings{huo2023bias,
  title={Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes},
  author={Huo, Dongyan (Lucy) and Chen, Yudong and Xie, Qiaomin},
  booktitle={ACM Sigmetrics},
  year={2023},
  month={June},
  arxiv={https://arxiv.org/abs/2210.00953}
}

@inproceedings{qin2023distributed,
  title={Distributed Threshold-based Offloading for Heterogeneous Mobile Edge Computing},
  author={Qin, Xudong and Xie, Qiaomin and Li, Bin},
  booktitle={International Conference on Distributed Computing Systems (ICDCS)},
  year={2023},
  month={July},
  url={https://www.computer.org/csdl/proceedings-article/icdcs/2023/398600a202/1R9N24DVYRi}
}

@inproceedings{zhang2023average,
  title={Sharper Model-free Reinforcement Learning for Average-reward Markov Decision Processes},
  author={Zhang, Zihan and Xie, Qiaomin},
  booktitle={Conference on Learning Theory (COLT)},
  year={2023},
  month={July},
  arxiv={https://arxiv.org/abs/2306.16394},
}

@inproceedings{mukherjee2023speed,
  title={{SPEED}: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits},
  author={Mukherjee, Subhojyoti and Xie, Qiaomin and Hanna, Josiah and Nowak, Robert},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  journal={arXiv preprint arXiv:2301.12357},
  year={2024},
  month={May},
  arxiv={https://arxiv.org/abs/2301.12357}
}

@inproceedings{mukherjee2023multi_task,
  title={Multi-task Representation Learning for Pure Exploration in Bilinear Bandits},
  author={Mukherjee, Subhojyoti and Xie, Qiaomin and Hanna, Josiah and Nowak, Robert},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  month={December},
  arxiv={https://arxiv.org/abs/2311.00327}
}

@inproceedings{hong2023restless,
      title={Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption}, 
      author={Yige Hong and Qiaomin Xie and Yudong Chen and Weina Wang},
      year={2023},
      month={December},
      eprint={2306.00196},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      arxiv={https://arxiv.org/abs/2306.00196},
      %journal={arXiv preprint arXiv:2306.00196},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS), Spotlight, },
      year={2023}
}



@inproceedings{wu2023faking,
      title={Data Poisoning to Fake a Nash Equilibrium in Markov Games}, 
      author={Young Wu and Jeremy McMahan and Xiaojin Zhu and Qiaomin Xie},
      year={2024},
      month={February},
      eprint={2306.08041},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      arxiv={https://arxiv.org/abs/2306.08041},
      %journal={arXiv preprint arXiv:2306.08041},
      booktitle={AAAI Conference on Artificial Intelligence}
}


@inproceedings{vlatakisgkaragkounis2023stochastic,
      title={Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements}, 
      author={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Angeliki Giannou and Yudong Chen and Qiaomin Xie},
      year={2024},
      month={May},
      booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS), Oral},
      eprint={2306.16502},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      arxiv={https://arxiv.org/abs/2306.16502},
      journal={arXiv preprint arXiv:2306.16502}
}

@inproceedings{huo2024effectiveness,
      title={Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference}, 
      author={Dongyan (Lucy) Huo and Yudong Chen and Qiaomin Xie},
      year={2024},
      month={February},
      booktitle={AAAI Conference on Artificial Intelligence},
      arxiv={https://arxiv.org/abs/2312.10894},
      %abstract={In this paper, we study the effectiveness of using a constant stepsize in statistical inference via linear stochastic approximation (LSA) algorithms with Markovian data. After establishing a Central Limit Theorem (CLT), we develop an inference procedure that uses the averaged LSA iterates to construct confidence intervals (CIs). Our procedure leverages the fast mixing property of constant-stepsize LSA for better covariance estimation, and employs Richardson-Romberg (RR) extrapolation to reduce the bias induced by a constant stepsize and Markovian data. We develop theoretical results for guiding stepsize selection in RR extrapolation, and identify several important settings where the bias provably vanishes even without extrapolation. We conduct extensive numerical experiments and compare against the classical diminishing stepsize regime. Our results show that using a constant stepsize enjoys easy hyperparameter tuning, fast convergence and consistently better CI coverage, especially with limited data.}
}

@inproceedings{chen2024exact_heavy_tailed,
      title={Exact Policy Recovery in Offline {RL} with Both Heavy-Tailed Rewards and Data Corruption}, 
      author={Yiding Chen and Xuezhou Zhang and Qiaomin Xie and Xiaojin Zhu},
      year={2024},
      month={February},
      booktitle={AAAI Conference on Artificial Intelligence},
      url={https://ojs.aaai.org/index.php/AAAI/article/view/29022}
}

@inproceedings{mcmahan2024attack_defense,
      title={Optimal Attack and Defense for Reinforcement Learning}, 
      author={Jeremy McMahan and Young Wu and Xiaojin Zhu and Qiaomin Xie},
      year={2024},
      month={February},
      booktitle={AAAI Conference on Artificial Intelligence},
      arxiv={https://arxiv.org/abs/2312.00198},
      eprint={2312.00198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      %journal={arXiv preprint arXiv:2312.00198}
}

@inproceedings{zhang2024nonsmooth,
  title={Prelimit Coupling and Steady-State Convergence of Constant-stepsize Nonsmooth Contractive Stochastic Approximation},
  author={Zhang, Yixuan and Huo, Dongyan (Lucy) and Chen, Yudong and Xie, Qiaomin},
  %journal={arXiv preprint arXiv:2404.06023},
  arxiv={https://arxiv.org/abs/2404.06023},
  booktitle={ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
  year={2024},
  month={June}
}

@inproceedings{mcmahan2024roping,
  title={Roping in Uncertainty: Robustness and Regularization in Markov Games},
  author={Jeremy McMahan and Giovanni Artiglio and Qiaomin Xie},
  booktitle={International Conference on Machine Learning (ICML)}, 
  arxiv={https://arxiv.org/abs/2406.08847}, 
  year={2024},
  month={July}
}

@inproceedings{wu2024minimally,
  title={Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value},
  author={Young Wu and Jeremy McMahan and Yiding Chen and Yudong Chen and Xiaojin Zhu and Qiaomin Xie},
  journal={arXiv preprint arXiv:2311.00582},
  arxiv={https://arxiv.org/abs/2311.00582},
  booktitle={International Conference on Machine Learning (ICML)},  
  year={2024},
  month={July}
}


@inproceedings{pavse2024stable,
      title={Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces}, 
      author={Brahma S. Pavse and Matthew Zurek and Yudong Chen and Qiaomin Xie and Josiah P. Hanna},
      arxiv={https://arxiv.org/abs/2306.01896},
      journal={arXiv preprint arXiv:2306.01896},
      booktitle={International Conference on Machine Learning (ICML)},  
  year={2024},
  month={July}
}


@inproceedings{zhang2024constant,
  title={Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation},
  author={Zhang, Yixuan and Xie, Qiaomin},
  arxiv={https://arxiv.org/abs/2401.13884},
  journal={arXiv preprint arXiv:2401.13884},
  booktitle={Reinforcement Learning Conference (RLC)},  
  year={2024},
  month={August}
}

@inproceedings{mcmahan2024inception,
  title={Inception: Efficiently Computable Misinformation Attacks on Markov Games},
  author={Jeremy McMahan and Young Wu and Yudong Chen and Xiaojin Zhu and Qiaomin Xie},
  journal={arXiv preprint arXiv:2307.09652},
  arxiv={https://arxiv.org/abs/2307.09652},
  booktitle={Reinforcement Learning Conference (RLC)},  
  year={2024},
  month={August}
}

@article{hong2024unichain,
  title={Unichain and aperiodicity are sufficient for asymptotic optimality of average-reward restless bandits},
  author={Hong, Yige and Xie, Qiaomin and Chen, Yudong and Wang, Weina},
  journal={arXiv preprint arXiv:2402.05689},
  arxiv={https://arxiv.org/abs/2402.05689},
  year={2024},
  month={December}
}

@inproceedings{huo2024collusion,
  title={The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize},
  author={Huo, Dongyan (Lucy) and Zhang, Yixuan and Chen, Yudong and Xie, Qiaomin},
  journal={arXiv preprint arXiv:2405.16732},
  arxiv={https://arxiv.org/abs/2405.16732},
  year={2024},
  month={December},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS), Spotlight,}
}

@article{mukherjee2024pretraining,
  title={Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning},
  author={Mukherjee, Subhojyoti and Hanna, Josiah P and Xie, Qiaomin and Nowak, Robert},
  journal={arXiv preprint arXiv:2406.05064},
  arxiv={https://arxiv.org/abs/2406.05064},
  year={2024},
  month={December}
}

@inproceedings{xiang2025coupling,
      title={Coupling-based Convergence Diagnostic and Stepsize Scheme for Stochastic Gradient Descent}, 
      author={Xiang Li and Qiaomin Xie},
      year={2025},
      month={February},
      booktitle={AAAI Conference on Artificial Intelligence}
}

@inproceedings{zhang2025subquadratic,
  title={A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression},
  author={Zhang, Yixuan and Huo, Dongyan (Lucy) and Chen, Yudong and Xie, Qiaomin},
  booktitle={ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
  year={2025},
  month={June},
  arxiv={https://arxiv.org/abs/2504.08178}
}


@inproceedings{kwon2025twotime,
      title={Two-Timescale Linear Stochastic Approximation: Constant Stepsizes Go a Long Way}, 
      author={Kwon, Jeongyeol and Dotson, Luke and Chen, Yudong and Xie, Qiaomin},
      year={2025},
      month={May},
      booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
      eprint={2410.13067},
      archivePrefix={arXiv},
      primaryClass={eess.SY},
      arxiv={https://arxiv.org/abs/2410.13067}
}
