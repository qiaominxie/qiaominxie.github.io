---
---

@string{aps = {American Physical Society,}}

@article{lu2011join,
  title={Join-idle-queue: A novel load balancing algorithm for dynamically scalable web services},
  author={Lu, Yi and Xie, Qiaomin and Kliot, Gabriel and Geller, Alan and Larus, James R and Greenberg, Albert},
  journal={Performance Evaluation},
  volume={68},
  number={11},
  pages={1056--1071},
  year={2011},
  publisher={North-Holland}
}

@inproceedings{xie2012degree,
  title={Degree-guided map-reduce task assignment with data locality constraint},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2012 IEEE International Symposium on Information Theory Proceedings},
  pages={985--989},
  year={2012},
  organization={IEEE}
}

@inproceedings{xie2015power,
  title={Power of d Choices for Large-Scale Bin Packing: A Loss Model},
  author={Xie, Qiaomin and Dong, Xiaobo and Lu, Yi and Srikant, R},
  booktitle={ACM Sigmetrics},
  year={2015}
}

@inproceedings{xie2015priority,
  title={Priority algorithm for near-data scheduling: Throughput and heavy-traffic optimality},
  author={Xie, Qiaomin and Lu, Yi},
  booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)},
  pages={963--972},
  year={2015},
  organization={IEEE}
}

@inproceedings{xie2016scheduling,
  title={Scheduling with Multi-level Data Locality: Throughput and Heavy-Traffic Optimality},
  author={Xie, Qiaomin and Yekkehkhany, Ali and Lu, Yi},
  booktitle={2016 IEEE Conference on Computer Communications (INFOCOM)},
  year={2016}
}

@article{xie2016pandas,
  title={Pandas: robust locality-aware scheduling with stochastic delay optimality},
  author={Xie, Qiaomin and Pundir, Mayank and Lu, Yi and Abad, Cristina L and Campbell, Roy H},
  journal={IEEE/ACM Transactions on Networking},
  volume={25},
  number={2},
  pages={662--675},
  year={2016},
  publisher={IEEE}
}

@inproceedings{gupta2017stochastic,
  title={Stochastic online scheduling on unrelated machines},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  booktitle={International Conference on Integer Programming and Combinatorial Optimization},
  pages={228--240},
  year={2017},
  organization={Springer, Cham}
}


@article{shah2017centralized,
  title={Centralized Congestion Control and Scheduling in a Datacenter},
  author={Shah, Devavrat and Xie, Qiaomin},
  journal={arXiv preprint arXiv:1710.02548},
  year={2017}
}

@inproceedings{shah2018q,
  title={Q-learning with nearest neighbors},
  author={Shah, Devavrat and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@inproceedings{liu2019reinforcement,
  title={Reinforcement learning for optimal control of queueing systems},
  author={Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={663--670},
  year={2019},
  organization={IEEE}
}

@inproceedings{fei2020dynamic,
  title={Dynamic Regret of Policy Optimization in Non-Stationary Environments},
  author={Fei, Yingjie and Yang, Zhuoran and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{mao2020poly,
  title={POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis},
  author={Mao, Weichao and Zhang, Kaiqing and Xie, Qiaomin and Basar, Tamer},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{fei2020risk,
  title={Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret},
  author={Fei, Yingjie and Yang, Zhuoran and Chen, Yudong and Wang, Zhaoran and Xie, Qiaomin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{shah2020stable,
  title={Stable Reinforcement Learning with Unbounded State Space},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={Learning for Dynamics and Control (L4DC)},
  pages={581},
  year={2020},
  arxiv={https://arxiv.org/abs/2006.04353}
}

@inproceedings{shah2020reinforcement,
  title={On Reinforcement Learning for Turn-based Zero-sum Markov Games},
  author={Shah, Devavrat and Somani, Varun and Xie, Qiaomin and Xu, Zhi},
  booktitle={Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  pages={pp--139},
  year={2020}
}


@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on Learning Theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@inproceedings{shah2020non,
  title={Non-asymptotic analysis of Monte Carlo tree search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle={ACM Sigmetrics},
  pages={31--32},
  year={2020}
}

@article{gupta2020greed,
  title={Greed works—online algorithms for unrelated machine stochastic scheduling},
  author={Gupta, Varun and Moseley, Benjamin and Uetz, Marc and Xie, Qiaomin},
  journal={Mathematics of operations research},
  volume={45},
  number={2},
  pages={497--516},
  year={2020},
  publisher={INFORMS}
}

@inproceedings{xie2021learning,
  title={Learning While Playing in Mean-Field Games: Convergence and Optimality},
  author={Xie, Qiaomin and Yang, Zhuoran and Wang, Zhaoran and Minca, Andreea},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={11436--11447},
  year={2021},
  organization={PMLR},
  html={https://proceedings.mlr.press/v139/xie21g.html}
}


@inproceedings{wang2021zero,
  title={Zero queueing for multi-server jobs},
  author={Wang, Weina and Xie, Qiaomin and Harchol-Balter, Mor},
  booktitle={ACM Sigmetrics},
  pages={13--14},
  year={2021}
}

@article{archer2022orsuite,
author = {Archer, Christopher and Banerjee, Siddhartha and Cortez, Mayleen and Rucker, Carrie and Sinclair, Sean R. and Solberg, Max and Xie, Qiaomin and Lee Yu, Christina},
title = {ORSuite: Benchmarking Suite for Sequential Operations Models},
year = {2022},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {2},
issn = {0163-5999},
url = {https://doi.org/10.1145/3512798.3512819},
doi = {10.1145/3512798.3512819},
abstract = {Reinforcement learning (RL) has received widespread attention across multiple communities, but the experiments have focused primarily on large-scale game playing and robotics tasks. In this paper we introduce ORSuite, an open-source library containing environments, algorithms, and instrumentation for operational problems. Our package is designed to motivate researchers in the reinforcement learning community to develop and evaluate algorithms on operational tasks, and to consider the true multi-objective nature of these problems by considering metrics beyond cumulative reward.},
journal = {SIGMETRICS Performance Evaluation Review},
month = {jan},
pages = {57--61},
numpages = {5}
}

@article{shah2022non_OR,
  title={Nonasymptotic Analysis of {Monte Carlo} Tree Search},
  author={Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  journal={Operations Research},
  volume={70},
  number={6},
  pages={3234--3260},
  year={2022},
  publisher={INFORMS},
  doi={10.1287/opre.2021.2239},
  html={https://pubsonline.informs.org/doi/abs/10.1287/opre.2021.2239?journalCode=opre}
}

@article{Liu2022rlqn,
author = {Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
title = {RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3529375},
doi = {10.1145/3529375},
abstract = {With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called RL for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing, and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively.},
journal = {ACM Transactions on Modeling and Performance Evaluation of Computing Systems},
month = {aug},
articleno = {2},
numpages = {35},
keywords = {Queueing networks, reinforcement learning}
}

@article{xie2023cce_mor,
author = {Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
title = {Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium},
year = {2023},
issue_date = {February 2023},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {48},
number = {1},
issn = {0364-765X},
url = {https://doi.org/10.1287/moor.2022.1268},
doi = {10.1287/moor.2022.1268},
abstract = {We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an O~(d3H3T) upper bound on the duality gap and regret, where d is the linear dimension, H the horizon and T the total number of timesteps. Our results do not require additional assumptions on the sampling model. Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a coarse correlated equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right.Funding: Q. Xie is partially supported by the National Science Foundation [Grant CNS-1955997] and by J.P. Morgan. Y. Chen is partially supported by the National Science Foundation [Grants CCF-1657420, CCF-1704828, and CCF-2047910]. Z. Wang acknowledges the National Science Foundation [Grants 2048075, 2008827, 2015568, and 1934931], the Simons Institute (Theory of Reinforcement Learning), Amazon, J.P. Morgan, and Two Sigma for their support.},
journal = {Mathematics of Operations Research},
month = {feb},
pages = {433–462},
numpages = {30},
keywords = {Markov games, function approximation, correlated equilibrium, secondary: 91A15, Primary: 68T05, reinforcement learning}
}